{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vPT-kafk5Lve"
      },
      "outputs": [],
      "source": [
        "# Toy Dataset\n",
        "data = [\n",
        "    (\"I love this product\", 1),\n",
        "    (\"This is the worst thing ever\", 0),\n",
        "    (\"I am so happy with this purchase\", 1),\n",
        "    (\"This is not what I expected\", 0),\n",
        "    (\"Absolutely fantastic!\", 1),\n",
        "    (\"Terrible experience\", 0),\n",
        "    (\"I will never buy this again\", 0),\n",
        "    (\"Best decision ever\", 1),\n",
        "    (\"I regret buying this\", 0),\n",
        "    (\"Such a great quality\", 1)\n",
        "]\n",
        "\n",
        "# Create two categories: 1 for positive sentiment, 0 for negative sentiment\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Z8CoHBF05O69"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Tokenization and Vocabulary Building\n",
        "def tokenize_sentence(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = Counter()\n",
        "for sentence, label in data:\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    vocab.update(tokens)\n",
        "\n",
        "# Create a word to index mapping\n",
        "word2idx = {word: idx for idx, (word, _) in enumerate(vocab.items(), 1)}\n",
        "word2idx['<PAD>'] = 0  # Add padding token\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# Convert sentences to sequences of indices\n",
        "def encode_sentence(sentence, word2idx, max_len=10):\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    encoded = [word2idx.get(token, 0) for token in tokens]  # Convert words to indices\n",
        "    # Padding to max length\n",
        "    encoded = encoded[:max_len] + [0] * (max_len - len(encoded))\n",
        "    return encoded\n",
        "\n",
        "# Encode data\n",
        "X = np.array([encode_sentence(sentence, word2idx) for sentence, _ in data])\n",
        "y = np.array([label for _, label in data])\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "R_H8OkQi5TLL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = SentenceDataset(X_train, y_train)\n",
        "test_dataset = SentenceDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2)\n"
      ],
      "metadata": {
        "id": "83HY75jv5WJ3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = embed_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        # Compute the attention-weighted sum of the values\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output, attn_weights\n"
      ],
      "metadata": {
        "id": "unlCSDHW5YnO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super(SentenceClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.attention = SelfAttention(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed the input tokens\n",
        "        embedded = self.embedding(x)\n",
        "\n",
        "        # Apply self-attention\n",
        "        attention_output, attn_weights = self.attention(embedded)\n",
        "\n",
        "        # Sum the attention outputs across the sequence\n",
        "        pooled_output = attention_output.mean(dim=1)\n",
        "\n",
        "        # Pass the pooled output through a fully connected layer\n",
        "        output = self.fc(pooled_output)\n",
        "        return output, attn_weights\n",
        "\n",
        "# Hyperparameters\n",
        "embed_dim = 16\n",
        "num_classes = 2\n",
        "\n",
        "# Initialize the model\n",
        "model = SentenceClassifier(vocab_size=len(word2idx), embed_dim=embed_dim, num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "WMiv9b9-5aj1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 40\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output, _ = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIeEWymV5ciK",
        "outputId": "8a1eefb4-bdfe-4a6a-d4cb-b9ae905b2a85"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8256252855062485\n",
            "Epoch 2, Loss: 0.7898856401443481\n",
            "Epoch 3, Loss: 0.7559326142072678\n",
            "Epoch 4, Loss: 0.7259760200977325\n",
            "Epoch 5, Loss: 0.7064915597438812\n",
            "Epoch 6, Loss: 0.6770203560590744\n",
            "Epoch 7, Loss: 0.6615673005580902\n",
            "Epoch 8, Loss: 0.6490078121423721\n",
            "Epoch 9, Loss: 0.6392672806978226\n",
            "Epoch 10, Loss: 0.6230557560920715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        output, _ = model(batch_x)\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, all_preds)\n",
        "print(f'Accuracy on the test set: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCaCtJxs5erl",
        "outputId": "18d0d4ef-20e3-43a9-aab3-72b485d4f718"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Attention for a Single Example\n",
        "def visualize_attention(sentence):\n",
        "    model.eval()\n",
        "    encoded_sentence = torch.LongTensor(encode_sentence(sentence, word2idx)).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output, attn_weights = model(encoded_sentence)\n",
        "\n",
        "    attn_weights = attn_weights.squeeze().cpu().numpy()\n",
        "\n",
        "    print(f'Attention Weights for Sentence: \"{sentence}\"')\n",
        "    print(attn_weights)\n",
        "\n",
        "# Example\n",
        "visualize_attention(\"I love this product\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrygyM595hFz",
        "outputId": "35933eae-fe24-4b8e-fd8d-0be0b4d90e7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights for Sentence: \"I love this product\"\n",
            "[[0.02559064 0.04033669 0.06305679 0.06938002 0.13360597 0.13360597\n",
            "  0.13360597 0.13360597 0.13360597 0.13360597]\n",
            " [0.06341338 0.09224927 0.05729984 0.11789563 0.11152366 0.11152366\n",
            "  0.11152366 0.11152366 0.11152366 0.11152366]\n",
            " [0.10491624 0.06680733 0.0967507  0.06556741 0.11099305 0.11099305\n",
            "  0.11099305 0.11099305 0.11099305 0.11099305]\n",
            " [0.10332625 0.12965928 0.05728712 0.13165252 0.0963458  0.0963458\n",
            "  0.0963458  0.0963458  0.0963458  0.0963458 ]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]\n",
            " [0.03381648 0.07661147 0.09586184 0.09351337 0.11669948 0.11669948\n",
            "  0.11669948 0.11669948 0.11669948 0.11669948]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Dataset preparation\n",
        "data = [\n",
        "    (\"I love this product\", 1),\n",
        "    (\"This is the worst thing ever\", 0),\n",
        "    (\"I am so happy with this purchase\", 1),\n",
        "    (\"This is not what I expected\", 0),\n",
        "    (\"Absolutely fantastic!\", 1),\n",
        "    (\"Terrible experience\", 0),\n",
        "    (\"I will never buy this again\", 0),\n",
        "    (\"Best decision ever\", 1),\n",
        "    (\"I regret buying this\", 0),\n",
        "    (\"Such a great quality\", 1)\n",
        "]\n",
        "\n",
        "# Tokenization and Vocabulary\n",
        "def tokenize_sentence(sentence):\n",
        "    return sentence.lower().split()\n",
        "\n",
        "vocab = Counter()\n",
        "for sentence, _ in data:\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    vocab.update(tokens)\n",
        "\n",
        "word2idx = {word: idx for idx, (word, _) in enumerate(vocab.items(), 1)}\n",
        "word2idx['<PAD>'] = 0  # Padding token\n",
        "\n",
        "vocab_size = len(word2idx)\n",
        "\n",
        "# Encode sentences to indices\n",
        "def encode_sentence(sentence, word2idx, max_len=10):\n",
        "    tokens = tokenize_sentence(sentence)\n",
        "    encoded = [word2idx.get(token, 0) for token in tokens]\n",
        "    return encoded[:max_len] + [0] * (max_len - len(encoded))\n",
        "\n",
        "# Encode data\n",
        "X = np.array([encode_sentence(sentence, word2idx) for sentence, _ in data])\n",
        "y = np.array([label for _, label in data])\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset and DataLoader\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = SentenceDataset(X_train, y_train)\n",
        "test_dataset = SentenceDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2)\n",
        "\n",
        "# Self-Attention Layer\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.scale = embed_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output, attn_weights\n",
        "\n",
        "# Model\n",
        "class SentenceClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        super(SentenceClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.attention = SelfAttention(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        attention_output, _ = self.attention(embedded)\n",
        "        pooled_output = attention_output.mean(dim=1)\n",
        "        output = self.fc(pooled_output)\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "embed_dim = 100\n",
        "num_classes = 2\n",
        "model = SentenceClassifier(vocab_size=len(word2idx), embed_dim=embed_dim, num_classes=num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 40  # Increase number of epochs for more robust training\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_x)\n",
        "        loss = criterion(output, batch_y)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        output = model(batch_x)\n",
        "        _, predicted = torch.max(output, dim=1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(y_test, all_preds)\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S6jBFse5jMG",
        "outputId": "f88522ae-95e8-4357-a3c9-38c69bcb0eed"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.8533\n",
            "Epoch 2, Loss: 0.6413\n",
            "Epoch 3, Loss: 0.5605\n",
            "Epoch 4, Loss: 0.4628\n",
            "Epoch 5, Loss: 0.3663\n",
            "Epoch 6, Loss: 0.2807\n",
            "Epoch 7, Loss: 0.1947\n",
            "Epoch 8, Loss: 0.1206\n",
            "Epoch 9, Loss: 0.0803\n",
            "Epoch 10, Loss: 0.0477\n",
            "Epoch 11, Loss: 0.0268\n",
            "Epoch 12, Loss: 0.0186\n",
            "Epoch 13, Loss: 0.0127\n",
            "Epoch 14, Loss: 0.0090\n",
            "Epoch 15, Loss: 0.0072\n",
            "Epoch 16, Loss: 0.0059\n",
            "Epoch 17, Loss: 0.0048\n",
            "Epoch 18, Loss: 0.0043\n",
            "Epoch 19, Loss: 0.0037\n",
            "Epoch 20, Loss: 0.0033\n",
            "Epoch 21, Loss: 0.0030\n",
            "Epoch 22, Loss: 0.0027\n",
            "Epoch 23, Loss: 0.0025\n",
            "Epoch 24, Loss: 0.0023\n",
            "Epoch 25, Loss: 0.0022\n",
            "Epoch 26, Loss: 0.0020\n",
            "Epoch 27, Loss: 0.0019\n",
            "Epoch 28, Loss: 0.0018\n",
            "Epoch 29, Loss: 0.0017\n",
            "Epoch 30, Loss: 0.0016\n",
            "Epoch 31, Loss: 0.0015\n",
            "Epoch 32, Loss: 0.0015\n",
            "Epoch 33, Loss: 0.0014\n",
            "Epoch 34, Loss: 0.0013\n",
            "Epoch 35, Loss: 0.0013\n",
            "Epoch 36, Loss: 0.0012\n",
            "Epoch 37, Loss: 0.0012\n",
            "Epoch 38, Loss: 0.0011\n",
            "Epoch 39, Loss: 0.0011\n",
            "Epoch 40, Loss: 0.0010\n",
            "Test Accuracy: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9eRujggH6AXb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}